{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import Row,SQLContext\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.Builder() \\\n",
    "     .appName(\"Realtime\") \\\n",
    "     .master(\"spark://spark-master-1:7077\") \\\n",
    "     .config(\"spark.jars\", \"./spark-streaming-kafka-0-8-assembly_2.11-2.0.0-preview.jar\") \\\n",
    "     .getOrCreate()\n",
    "sc = ss.sparkContext\n",
    "ssc = StreamingContext(sc, 2)\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "ss.sparkContext.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brokers = 'kafka-1:9092,kafka-2:9092'\n",
    "topic='tweet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(x):\n",
    "    if x[0] == '#':\n",
    "        if len(x) != 1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lines = ks.window(30,10)\n",
    "lines = ks.map(lambda x: json.loads(x[1])['text'] )\n",
    "words = lines.flatMap(lambda line: line.split())\n",
    "hashtags = words.filter(lambda w: filter(w)).map(lambda x: (x, 1))\n",
    "tags_totals = hashtags.reduceByKey(lambda a, b: a+b)\\\n",
    "                .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False))\n",
    "tags_totals.foreachRDD(process_rdd)    \n",
    "ssc.start()\n",
    "time.sleep(1000)\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentAnalysis(ssc):\n",
    "    es = Elasticsearch()\n",
    "    es_write_conf = { \"es.resource\" : \"tsd/mem\",\"es.input.json\" : \"true\" } \n",
    "    kstream = KafkaUtils.createDirectStream(ssc, [topic], kafkaParams={\"metadata.broker.list\":brokers})\n",
    "    tweets  = kstream.map(lambda line: json.loads(line[1]))\n",
    "    sentiAnalysis = tweets.map(lambda line: (line,classify(line[\"text\"])))\n",
    "    result2 = result1.map(lambda line: (None, line))\n",
    "    result2.foreachRDD(lambda line : line.saveAsNewAPIHadoopFile(\n",
    "            path=\"-\", \n",
    "            outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "            keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "            valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "            conf=es_write_conf))\n",
    "\tssc.start()\n",
    "\tssc.awaitTermination()\n",
    "\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
